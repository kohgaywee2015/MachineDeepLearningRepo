{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo, make a function from the two codes for loops, they are very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel as parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the movies.dat dataset, user.dat dataset, rating.dat dataset\n",
    "movies_df = pd.read_csv(\"../archive/ml-1m/movies.dat\", \n",
    "                           sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n",
    "\n",
    "users_df = pd.read_csv(\"../archive/ml-1m/users.dat\", \n",
    "                           sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n",
    "\n",
    "ratings_df = pd.read_csv(\"../archive/ml-1m/ratings.dat\", \n",
    "                           sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n",
    "\n",
    "# Preparing the training set and testing set\n",
    "training_set = pd.read_csv(\"../archive/ml-100k/u1.base\", delimiter=\"\\t\")\n",
    "training_set_array = np.array(training_set, dtype='int')\n",
    "\n",
    "testing_set = pd.read_csv(\"../archive/ml-100k/u1.test\", delimiter=\"\\t\")\n",
    "testing_set_array = np.array(testing_set, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the total number of users and movies\n",
    "# The last user is user 943 and the last movie is the movie 1330\n",
    "\n",
    "# Performing the double max to get the max from the training set and testing set\n",
    "nb_users = int(max(max(training_set_array[:, 0]), max(testing_set_array[:,0]))) # All rows for column 0\n",
    "nb_movies = int(max(max(training_set_array[:, 1]), max(testing_set_array[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into an array with users in rows and movies in columns\n",
    "# We will have to create a list with the sublist being every user with info. on their ratings for each movies\n",
    "\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users+1):\n",
    "        id_movies = data[:, 1][data[:,0] == id_users] # the 2nd column represents the movies and only for the i user\n",
    "        id_ratings = data[:, 2][data[:,0] == id_users] # the 3rd column represents the rating for the i user\n",
    "        \n",
    "        # Initilializing an array with all the possible movies\n",
    "        ratings = np.zeros(nb_movies) \n",
    "        \n",
    "        # If we find the movie, we can replace the rating to the 0\n",
    "        ratings[id_movies-1] = id_ratings\n",
    "        \n",
    "        # Creating a large list that contains all the information\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the convert function to the training and testing set\n",
    "\n",
    "training_set_array = convert(training_set_array)\n",
    "testing_set_array = convert(testing_set_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the arrays into Tensors\n",
    "training_set_array = torch.FloatTensor(training_set_array)\n",
    "testing_set_array = torch.FloatTensor(testing_set_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the archiecture of the neural network (stacked auto encoders)\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        \n",
    "        # We are using the superfunction to get the inheritance from nn.Module class\n",
    "        super(SAE, self).__init__()\n",
    "        \n",
    "        # Keep in mind, the 20 nodes that are used the first output layer, was calc. by the author trail and error\n",
    "        # One of the 20 nodes could be a horror movies genre, thus, a nn could learn that a user loves horror movies\n",
    "        # When the nn attempts to predict movies, if the user has a favorable look to horror movies\n",
    "        # so the nn at this step will be activated when a horror movies shows up\n",
    "        \n",
    "        # Also, every variabe must be connected with one another. As we see, the # of inputs connect to \n",
    "        # the # outputs in the next layer (common sense?)\n",
    "        \n",
    "        # As we go the nn, we are finding a more granular type of feature based from the previous layer\n",
    "        \n",
    "        # Also, for the third layer, I mentioned I am decoding. Meaning, I am recreating the autoencoder \n",
    "        # to provide the # of movies predicted, thus, it must be symmetrical\n",
    "        \n",
    "        # The activation function works as a sign of whether the nn should be activated based on the input from \n",
    "        # the user. E.g a person must like a certain amount of horror movies before the nn is activated\n",
    "        \n",
    "        self.fc1 = nn.Linear(nb_movies, 20) # 20 nodes are in the first hidden layer and nb_movies are used to find the predictions \n",
    "        self.fc2 = nn.Linear(20, 10) # 20 is the input, and 10 is the output from the hidden layer\n",
    "        self.fc3 = nn.Linear(10, 20) # 20 is the output, we are starting to decode\n",
    "        self.fc4 = nn.Linear(20, nb_movies) # the last full connection\n",
    "        self.activation = nn.Sigmoid() # activated with with a sigmoid function\n",
    "        \n",
    "        \n",
    "    # forward represent forward propogation\n",
    "    # the activation function is used through each step of the layer and checking which nodes should be activated\n",
    "    # based by the response of the user\n",
    "    \n",
    "    # Also noticed how the x is updated. Well, as proceed in the nn, we are updating what the rating of the movies\n",
    "    # and the final x is the our prediction of the movies based on what the nn learned\n",
    "        \n",
    "    def forward(self, x): # x is the input vector of features (with all the ratings of movies for a specific user)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "sae = SAE()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confusion:\n",
    "    - The total number of movies is inserted into the autoencoder... are we rating every single of these movies? \n",
    "    - Of course dummy. We have to figure what our model will predict for the movies we insert based on the algo that is build on the model. Thus, we need to insert every one of these movies, make a prediciton, and check how well our models does at prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.7721426468278383\n",
      "epoch: 2 loss: 1.0966669397249384\n",
      "epoch: 3 loss: 1.0533056132552618\n",
      "epoch: 4 loss: 1.0382905200946646\n",
      "epoch: 5 loss: 1.0309276614943297\n",
      "epoch: 6 loss: 1.0267279783142178\n",
      "epoch: 7 loss: 1.0241626594370508\n",
      "epoch: 8 loss: 1.0220179193553465\n",
      "epoch: 9 loss: 1.0208516102313108\n",
      "epoch: 10 loss: 1.019618077291776\n",
      "epoch: 11 loss: 1.018904396326807\n",
      "epoch: 12 loss: 1.018341789953803\n",
      "epoch: 13 loss: 1.0180642615068993\n",
      "epoch: 14 loss: 1.0176610987716836\n",
      "epoch: 15 loss: 1.0172370566377509\n",
      "epoch: 16 loss: 1.0170501330414188\n",
      "epoch: 17 loss: 1.0165847879140542\n",
      "epoch: 18 loss: 1.0165423313586974\n",
      "epoch: 19 loss: 1.0163735627745438\n",
      "epoch: 20 loss: 1.0161017672341948\n",
      "epoch: 21 loss: 1.0162266738700232\n",
      "epoch: 22 loss: 1.016090570773161\n",
      "epoch: 23 loss: 1.0158842624685263\n",
      "epoch: 24 loss: 1.0160723270439764\n",
      "epoch: 25 loss: 1.0155523835615834\n",
      "epoch: 26 loss: 1.015781173921301\n",
      "epoch: 27 loss: 1.0155908856347717\n",
      "epoch: 28 loss: 1.014939148401639\n",
      "epoch: 29 loss: 1.0122200408728812\n",
      "epoch: 30 loss: 1.0117148539697027\n",
      "epoch: 31 loss: 1.0090176658267278\n",
      "epoch: 32 loss: 1.0092089488145768\n",
      "epoch: 33 loss: 1.0049444626766466\n",
      "epoch: 34 loss: 1.00555467008753\n",
      "epoch: 35 loss: 1.0013390676200438\n",
      "epoch: 36 loss: 1.0000785012512428\n",
      "epoch: 37 loss: 0.9960052858271866\n",
      "epoch: 38 loss: 0.996025363318203\n",
      "epoch: 39 loss: 0.9963203790669587\n",
      "epoch: 40 loss: 0.9950200245868597\n",
      "epoch: 41 loss: 0.9902101834897862\n",
      "epoch: 42 loss: 0.9894996444246977\n",
      "epoch: 43 loss: 0.9884328048253944\n",
      "epoch: 44 loss: 0.9851596519003042\n",
      "epoch: 45 loss: 0.9813765064354157\n",
      "epoch: 46 loss: 0.9809177672299992\n",
      "epoch: 47 loss: 0.9779336272452908\n",
      "epoch: 48 loss: 0.9824726040634248\n",
      "epoch: 49 loss: 0.9786324602136736\n",
      "epoch: 50 loss: 0.9798942495214625\n",
      "epoch: 51 loss: 0.974551540139527\n",
      "epoch: 52 loss: 0.9800840238044726\n",
      "epoch: 53 loss: 0.9786082777169671\n",
      "epoch: 54 loss: 0.9819372981261462\n",
      "epoch: 55 loss: 0.9782959202392091\n",
      "epoch: 56 loss: 0.9828091699745253\n",
      "epoch: 57 loss: 0.9755979875522216\n",
      "epoch: 58 loss: 0.9780643141922294\n",
      "epoch: 59 loss: 0.9709433845270935\n",
      "epoch: 60 loss: 0.9753582444986891\n",
      "epoch: 61 loss: 0.9719554611381661\n",
      "epoch: 62 loss: 0.9637622724544076\n",
      "epoch: 63 loss: 0.9626036750546152\n",
      "epoch: 64 loss: 0.9658665062204791\n",
      "epoch: 65 loss: 0.964855603139553\n",
      "epoch: 66 loss: 0.9615877408362066\n",
      "epoch: 67 loss: 0.9604010390611996\n",
      "epoch: 68 loss: 0.9563887307505456\n",
      "epoch: 69 loss: 0.9546599428277645\n",
      "epoch: 70 loss: 0.9549036908191759\n",
      "epoch: 71 loss: 0.9534336792885576\n",
      "epoch: 72 loss: 0.9529838501383096\n",
      "epoch: 73 loss: 0.9505253504206201\n",
      "epoch: 74 loss: 0.9512369534230505\n",
      "epoch: 75 loss: 0.9472925228183985\n",
      "epoch: 76 loss: 0.9476175695834064\n",
      "epoch: 77 loss: 0.9452988370110289\n",
      "epoch: 78 loss: 0.9471136676427544\n",
      "epoch: 79 loss: 0.9439631955788993\n",
      "epoch: 80 loss: 0.9453667851918062\n",
      "epoch: 81 loss: 0.9426434945883269\n",
      "epoch: 82 loss: 0.9438333959416433\n",
      "epoch: 83 loss: 0.9413238231108364\n",
      "epoch: 84 loss: 0.9431069007834337\n",
      "epoch: 85 loss: 0.9399143592201531\n",
      "epoch: 86 loss: 0.9400286610504728\n",
      "epoch: 87 loss: 0.9387116635883342\n",
      "epoch: 88 loss: 0.9389666764282819\n",
      "epoch: 89 loss: 0.9375700973511402\n",
      "epoch: 90 loss: 0.9394041333942603\n",
      "epoch: 91 loss: 0.9356965064222167\n",
      "epoch: 92 loss: 0.9382597057381484\n",
      "epoch: 93 loss: 0.9351803491034719\n",
      "epoch: 94 loss: 0.9380921573273275\n",
      "epoch: 95 loss: 0.9353200121160887\n",
      "epoch: 96 loss: 0.9366750292641831\n",
      "epoch: 97 loss: 0.9336399378718164\n",
      "epoch: 98 loss: 0.9355292375864325\n",
      "epoch: 99 loss: 0.9327477011263675\n",
      "epoch: 100 loss: 0.9351861122397923\n",
      "epoch: 101 loss: 0.9320174281615251\n",
      "epoch: 102 loss: 0.9337872509946599\n",
      "epoch: 103 loss: 0.9314486123596524\n",
      "epoch: 104 loss: 0.9333836164858152\n",
      "epoch: 105 loss: 0.9313248556186577\n",
      "epoch: 106 loss: 0.9326441406982259\n",
      "epoch: 107 loss: 0.930190842636812\n",
      "epoch: 108 loss: 0.9315632159263747\n",
      "epoch: 109 loss: 0.9296376269514521\n",
      "epoch: 110 loss: 0.9310952222758812\n",
      "epoch: 111 loss: 0.9293805175655546\n",
      "epoch: 112 loss: 0.9310156651688439\n",
      "epoch: 113 loss: 0.9288286654804917\n",
      "epoch: 114 loss: 0.9302516956068368\n",
      "epoch: 115 loss: 0.9286941376598371\n",
      "epoch: 116 loss: 0.9301129485755445\n",
      "epoch: 117 loss: 0.9283653303999267\n",
      "epoch: 118 loss: 0.9294512389291455\n",
      "epoch: 119 loss: 0.9277942441968049\n",
      "epoch: 120 loss: 0.9288203444948927\n",
      "epoch: 121 loss: 0.9270857651302835\n",
      "epoch: 122 loss: 0.9280772918182579\n",
      "epoch: 123 loss: 0.9266393128478081\n",
      "epoch: 124 loss: 0.9283980381770822\n",
      "epoch: 125 loss: 0.9267861674804743\n",
      "epoch: 126 loss: 0.9269015255882765\n",
      "epoch: 127 loss: 0.9260116650615438\n",
      "epoch: 128 loss: 0.9265509021597457\n",
      "epoch: 129 loss: 0.9258685786515328\n",
      "epoch: 130 loss: 0.9264698800854503\n",
      "epoch: 131 loss: 0.9252432847070642\n",
      "epoch: 132 loss: 0.9258481959651508\n",
      "epoch: 133 loss: 0.9245632488714561\n",
      "epoch: 134 loss: 0.9256738621061231\n",
      "epoch: 135 loss: 0.9242171314412851\n",
      "epoch: 136 loss: 0.9251161388666654\n",
      "epoch: 137 loss: 0.9238366686495684\n",
      "epoch: 138 loss: 0.9242233523351218\n",
      "epoch: 139 loss: 0.9230234865418459\n",
      "epoch: 140 loss: 0.9239503718181392\n",
      "epoch: 141 loss: 0.9226556731261937\n",
      "epoch: 142 loss: 0.9238010897591536\n",
      "epoch: 143 loss: 0.9225925545655385\n",
      "epoch: 144 loss: 0.9234190485373229\n",
      "epoch: 145 loss: 0.9221506460411178\n",
      "epoch: 146 loss: 0.9227978008767984\n",
      "epoch: 147 loss: 0.9214886750986329\n",
      "epoch: 148 loss: 0.9224662839085781\n",
      "epoch: 149 loss: 0.9211022914026181\n",
      "epoch: 150 loss: 0.9219955362566682\n",
      "epoch: 151 loss: 0.9207032028691489\n",
      "epoch: 152 loss: 0.921612644660118\n",
      "epoch: 153 loss: 0.9203503040548864\n",
      "epoch: 154 loss: 0.9211647422012949\n",
      "epoch: 155 loss: 0.9199918939962346\n",
      "epoch: 156 loss: 0.9208861268795651\n",
      "epoch: 157 loss: 0.9195570864228269\n",
      "epoch: 158 loss: 0.9208346391891546\n",
      "epoch: 159 loss: 0.9190334707152066\n",
      "epoch: 160 loss: 0.9202019620846801\n",
      "epoch: 161 loss: 0.9189497696552622\n",
      "epoch: 162 loss: 0.9198818525187223\n",
      "epoch: 163 loss: 0.9182296503422646\n",
      "epoch: 164 loss: 0.9196965065855254\n",
      "epoch: 165 loss: 0.9180199342995633\n",
      "epoch: 166 loss: 0.9191179960921898\n",
      "epoch: 167 loss: 0.91758172231316\n",
      "epoch: 168 loss: 0.9184343901565662\n",
      "epoch: 169 loss: 0.9169857169509102\n",
      "epoch: 170 loss: 0.9182232370602909\n",
      "epoch: 171 loss: 0.9167394689834952\n",
      "epoch: 172 loss: 0.9178398147812548\n",
      "epoch: 173 loss: 0.9165073385410598\n",
      "epoch: 174 loss: 0.9180689240471531\n",
      "epoch: 175 loss: 0.9163133120571939\n",
      "epoch: 176 loss: 0.9172849566941881\n",
      "epoch: 177 loss: 0.9158266930637169\n",
      "epoch: 178 loss: 0.9171187273874738\n",
      "epoch: 179 loss: 0.9157953631996645\n",
      "epoch: 180 loss: 0.9169550409192649\n",
      "epoch: 181 loss: 0.9154493866111849\n",
      "epoch: 182 loss: 0.916761057838137\n",
      "epoch: 183 loss: 0.9154096945537031\n",
      "epoch: 184 loss: 0.9164432695375073\n",
      "epoch: 185 loss: 0.9151317732477154\n",
      "epoch: 186 loss: 0.9163230166311063\n",
      "epoch: 187 loss: 0.9145688436136509\n",
      "epoch: 188 loss: 0.9158260885886599\n",
      "epoch: 189 loss: 0.9142196066584939\n",
      "epoch: 190 loss: 0.9154233511965741\n",
      "epoch: 191 loss: 0.9142911206901948\n",
      "epoch: 192 loss: 0.9156137121295272\n",
      "epoch: 193 loss: 0.9139757383630539\n",
      "epoch: 194 loss: 0.9153910932279223\n",
      "epoch: 195 loss: 0.9136081525018455\n",
      "epoch: 196 loss: 0.9147443117380707\n",
      "epoch: 197 loss: 0.9145491491602918\n",
      "epoch: 198 loss: 0.9160248871290473\n",
      "epoch: 199 loss: 0.9131844124349\n",
      "epoch: 200 loss: 0.9152415526886657\n"
     ]
    }
   ],
   "source": [
    "# Training the SAE \n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "    train_loss = 0\n",
    "    s = 0. # The number of user that rated at least one movie\n",
    "    \n",
    "    for id_user in range(nb_users):\n",
    "        \n",
    "        # We have to an additional dimension (the batch) \n",
    "        # Using the Variable function, we can create an addtional dim\n",
    "        # The 0 is the index that this new dimenison will go in\n",
    "        input_from_user = Variable(training_set_array[id_user]).unsqueeze(0)\n",
    "        \n",
    "        # Cloning the input\n",
    "        target = input_from_user.clone()\n",
    "        \n",
    "        # We would only like to user that have rated movies...\n",
    "        # Checking the movies rated in target (a copy of the movies that user rated)\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            \n",
    "            # Next, we next to get a vector of the predicted ratings\n",
    "            output = sae(input_from_user)\n",
    "            \n",
    "            # Even though it seems common sense, we do not want to compute the gradient descent of the target\n",
    "            # we only need the gradient descent of the input variable\n",
    "            # this will make the code run quicker and more efficient\n",
    "            target.requires_grad = False\n",
    "            \n",
    "            # We need to reassure the code that we will have the movies that were not rated as 0 \n",
    "            # everything will be computed\n",
    "            output[target == 0] = 0\n",
    "            \n",
    "            # Computing the loss function\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # This represents the error average from the movies that were rated\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data>0) + 1e-10)\n",
    "            \n",
    "            # This could either be backward or forward, depending on the goal for the loss function\n",
    "            # in our case, we want to decrease the loss function, hence, we use backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating the loss function, the index 0 contains the error \n",
    "            # We also need to multiply it with the mean_corrector (for adjustment)\n",
    "            # np.sqrt for the one-degree loss\n",
    "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            \n",
    "            # Keep tracking of the users that rated the movie\n",
    "            s += 1.\n",
    "            \n",
    "            # Need to use the optimizer to find the best way to update the weights\n",
    "            # It decides the intensity of the amnt. that the weight will be updated\n",
    "            optimizer.step()\n",
    "            \n",
    "    # The train loss needs to the average for all the movies rated\n",
    "    print('epoch: {0} loss: {1}'.format(str(epoch), str(train_loss/s)))\n",
    "    \n",
    "    # We should a loss function less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9474370465403681\n"
     ]
    }
   ],
   "source": [
    "# Testing the SAE \n",
    "# Our goal is to predict whether a user will or will not like movies they have not watched\n",
    "# We then use the test (sometime in the future) where we get the actual ratings\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "\n",
    "    input_from_user = Variable(training_set_array[id_user]).unsqueeze(0)\n",
    "    target = Variable(testing_set_array[id_user])\n",
    "\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input_from_user)\n",
    "        target.requires_grad = False\n",
    "        output[target == 0] = 0\n",
    "\n",
    "        # Computing the loss function\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # This represents the error average from the movies that were rated\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data>0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "\n",
    "        s += 1.\n",
    "\n",
    "# The test loss needs to the average for all the movies rated\n",
    "print('test loss: {0}'.format(str(test_loss/s)))\n",
    "\n",
    "# We should a loss function less than 1\n",
    "\n",
    "# What does it mean? That if you were to predict the movie some rating (say 3 starts)... \n",
    "# The recommendaer would have predicted the rating btw 3 and 5 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autoencoder():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo, make a function from the two codes for loops, they are very similar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
