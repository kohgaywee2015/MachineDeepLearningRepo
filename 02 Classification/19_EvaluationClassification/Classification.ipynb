{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 19: Evaluations for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 126: False Positives and Negatives\n",
    "- False Positive (Type 1 error)\n",
    "    - We predicted a positive case but it was wrong\n",
    "    - We predicted a positive outcome but that was false\n",
    "- False Negative (Type 2 error)\n",
    "    - We predicted a negative outcome, but the opposite happened\n",
    "    - We predicted a negative outcome but that was false \n",
    "- Type 1 Error is less dangerous that a Type 2 Error\n",
    "- Type 1 error is a warning\n",
    "- Type 2 error is red flad\n",
    "- Example:\n",
    "    - Type 1 error could be that you predicted an earthquake that never happened\n",
    "    - Type 2 error could be that you did NOT predicte an earthquake and it HAPPENED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 127: Confusion Matrix\n",
    "- Look below for the code\n",
    "- **Accuracy Rate: Correct/Total**\n",
    "- **Error Rate: Wrong/Total**\n",
    "\n",
    "\n",
    "- PROBLEMS\n",
    "    - When you are data has more than 2 classes. With 3 or more classes you may get a classification accuracy of 80%, but you donâ€™t know if that is because all classes are being predicted equally well or whether one or two classes are being neglected by the model.\n",
    "    - When your data does not have an even number of classes. You may achieve accuracy of 90% or more, but this is not a good score if 90 records for every 100 belong to one class and you can achieve this score by always predicting the most common class value.\n",
    "        - Andrew Ng explains that if you have a sample size that is weighted towards one class, predicting that outcome might provide an accurate prediction\n",
    "    Precision: Out of all the patient that we predicted that have cancer (or 1), what fraction actually have cancer?\n",
    "\n",
    "\n",
    "- Alternatives\n",
    "    - **Precision**: Out of all the patient that we predicted that have cancer (or 1), what fraction actually have cancer?\n",
    "        - TRUE POSITIVES/PREDICTED POSITIVES -> TRUE POSITIVES/(TRUE POSITIVES + FALSE POSITIVES)\n",
    "        - Using the confusion matrix, this is represented as row 1\n",
    "    - **Recall**: Out of all the patient that actually have cancer (or 1), what fraction did we correctly detect as having cancer?\n",
    "        - TRUE POSITIVES/ACTUAL POSITIVES -> TRUE POSITIVES/(TRUE POSITIVES + FALSE NEGATIVES)\n",
    "        - Using the confusion matrix, this is represented as col. 1 \n",
    "    - **Trade-Off**\n",
    "        - For precision, we are using the values that we positive predictions and not all the predictions. So if we didn't predict ALL the positive values, if we did well on the predicitons we made, we have a good precision.\n",
    "        - However, recall looks at all the actual values (that are 1) and check our performance. IT doesn't care if predicted all the values 1. It would return a high recall score since we predicted 1 to most of the actual 1's.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred = np.array(np.random.randint(low=0, high=2, size=10))\n",
    "true = np.array(np.random.randint(low=0, high=2, size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pos: 1\n",
      "false pos: 2\n",
      "true neg: 3\n",
      "false neg: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(true, pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\n",
    "    \"true pos: {0}\\n\"\n",
    "    \"false pos: {1}\\n\"\n",
    "    \"true neg: {2}\\n\"\n",
    "    \"false neg: {3}\\n\".format(tp, fp, tn, fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 128: Confusion Matrix Paradox\n",
    "- Cannot based it on just accuracy because we might have better predictions if we only choose 1 class (the one with a larger sample)\n",
    "- <img src=\"../archive/AZMachineLearning_Per1.png\">\n",
    "- <img src=\"../archive/AZMachineLearning_Per2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 128: CAP Curve\n",
    "- The \"LINE\" is the average that convert based on historical info\n",
    "    - In the example: 10% answer our offers in our retail business\n",
    "    - <img src=\"../archive/AZMachineLearning_Per4.png\">\n",
    "- You can compare it with other models. The curve line can tell us how good are model is performing. The closer to the top, the better the model.\n",
    "- We have to perform better than our historic performance\n",
    "- The Crystal Model (the best one), would be that out of total contacted, (when we hit 10%), we get 10% of people to respond. We are using our time and money resourceful.\n",
    "- <img src=\"../archive/AZMachineLearning_Per5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP Curve VS. ROC Curve\n",
    "- <img src=\"../archive/AZMachineLearning_Per8.png\"alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "- The image below\n",
    "    - To plot the graohin the ROC curve, we use the False Positive Rate and True Positive Rate\n",
    "    - Thus, given a threshold, the TPR is the ratio of the true to the positive total (shown by the red graph)\n",
    "    - The FPR would be if any values from the Blue graph were to the right of the threshold. The ratio would be blue values to the right of the threshold out of all the negative total (shown by the blue graph)\n",
    "<img src=\"../archive/AZMachineLearning_Per9.png\"alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "- A GOOD ROC Curve is one that hugs the upper right corner\n",
    "<img src=\"../archive/AZMachineLearning_Per10.png\"alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "- A BAD ROC Curve is one that's similar to the linear line\n",
    "<img src=\"../archive/AZMachineLearning_Per11.png\"alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "- AUC: Area under the Curve\n",
    "    - Our goal is to have an AUC close to 1\n",
    "\n",
    "- **Important Notes**\n",
    "    - ROC Curves are useful even when your predictions are not \"properly caliberated\"\n",
    "    - ROC Curves can be extended to problems with three or more classes\n",
    "        - class 1 vs classes 2 & 3\n",
    "        - class 2 vs classes 1 & 3\n",
    "        - class 3 vs classes 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 129: CAP Analysis\n",
    "- We find the area of the Crystal Ball Model\n",
    "- We find the basic or average model\n",
    "- We fit our model within these constaints and evaulate a percentage compariing it to ht e total\n",
    "<img src=\"../archive/AZMachineLearning_Per6.png\"alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "- Another way to evaulate is using the 50% in the y-axis\n",
    "<img src=\"../archive/AZMachineLearning_Per7.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lecture 131: How do I know which model to choose for my problem ?\n",
    "\n",
    "Same as for regression models, you first need to figure out whether your problem is linear or non linear. You will learn how to do that in Part 10 - Model Selection. Then:\n",
    "\n",
    "If your problem is linear, you should go for Logistic Regression or SVM.\n",
    "\n",
    "If your problem is non linear, you should go for K-NN, Naive Bayes, Decision Tree or Random Forest.\n",
    "\n",
    "Then from a business point of view, you would rather use:\n",
    "\n",
    "- Logistic Regression or Naive Bayes when you want to rank your predictions by their probability. For example if you want to rank your customers from the highest probability that they buy a certain product, to the lowest probability. Eventually that allows you to target your marketing campaigns. And of course for this type of business problem, you should use Logistic Regression if your problem is linear, and Naive Bayes if your problem is non linear.\n",
    "\n",
    "- SVM when you want to predict to which segment your customers belong to. Segments can be any kind of segments, for example some market segments you identified earlier with clustering.\n",
    "\n",
    "- Decision Tree when you want to have clear interpretation of your model results,\n",
    "\n",
    "- Random Forest when you are just looking for high performance with less need for interpretation. \n",
    "\n",
    "\n",
    "- 2. How do I know which model to choose for my problem ?\n",
    "\n",
    "Same as for regression models, you first need to figure out whether your problem is linear or non linear. You will learn how to do that in Part 10 - Model Selection. Then:\n",
    "\n",
    "If your problem is linear, you should go for Logistic Regression or SVM.\n",
    "\n",
    "If your problem is non linear, you should go for K-NN, Naive Bayes, Decision Tree or Random Forest.\n",
    "\n",
    "Then from a business point of view, you would rather use:\n",
    "\n",
    "- Logistic Regression or Naive Bayes when you want to rank your predictions by their probability. For example if you want to rank your customers from the highest probability that they buy a certain product, to the lowest probability. Eventually that allows you to target your marketing campaigns. And of course for this type of business problem, you should use Logistic Regression if your problem is linear, and Naive Bayes if your problem is non linear.\n",
    "\n",
    "- SVM when you want to predict to which segment your customers belong to. Segments can be any kind of segments, for example some market segments you identified earlier with clustering.\n",
    "\n",
    "- Decision Tree when you want to have clear interpretation of your model results,\n",
    "\n",
    "- Random Forest when you are just looking for high performance with less need for interpretation. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
