{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat\n",
    "- Assumptions of a linear regression \n",
    "    - linearity\n",
    "    - homoscedasticity\n",
    "    - multivariate normality\n",
    "    - indepedence of error\n",
    "    - lack of multicollinearity\n",
    "    \n",
    "### Dummy Variables\n",
    "- Make sure that you do not include both dummy variables because this will make it a wrap. \n",
    "\n",
    "\n",
    "### Way to Build a Model\n",
    "- If you throw a lot of garbage, it will spit garbage out\n",
    "- You must explain why certain variables predict the behavior of the dependent variable\n",
    "- 5 Way To Build a Model\n",
    "    - **All-in**: \n",
    "        - Throw all in the variables, all the variables are the true predictors\n",
    "    - **Backward Elimination**: \n",
    "        - First, you must choose a signficance level you want to be under\n",
    "        - Fit the model with all of the variables\n",
    "        - Consider the predictor with the lowest p-value\n",
    "        - Remove the variables that have the highest p-value\n",
    "        - Recreate the model with a fewer number of models (which will change the coefficient and p-value)\n",
    "        - Continue doing this until you reach the conclusion that all the p-values are lower than the sigificant point\n",
    "    - **Forward Elimination**:\n",
    "        - Select a signficance for the treshold\n",
    "        - Fit all the independent regression with each variables and select the one with the lowest p-value\n",
    "        - Once you have the lowest predictor, check for all the possibilties with every other predictor (making it a 2 variable regression)\n",
    "        - Keep the one with the lowest p-value\n",
    "        - Repeat this process\n",
    "    - **Bidirectional Eliminiation**:\n",
    "        - It combines the two previous models\n",
    "        - Perform the next step in the forward model to enter new predictors\n",
    "        - Perform all the steps in the backward model and take out predictors that do not add any value\n",
    "    - **Score Comparision**:\n",
    "        - Select a criteria (like R-Squared)\n",
    "        - Construct all possible regression models $2^{N-1}$ total combination\n",
    "        - Select the one with the best criteria\n",
    "        \n",
    "- The bad thing is that when we have statistical signficance, the answer will be very black and white\n",
    "\n",
    "### Using Adjusted R-Squared for Robust Models\n",
    "- While just looking at the p-value can provide some value, we should also look at the way that the adjusted r-squared is either increasing or decreasing\n",
    "\n",
    "### Coefficients\n",
    "- For every (in this example) increase in $ 1 RD spend, it would increase profit by .79 cents\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
